# Deep Learning Specialization on Coursera

**Master Deep Learning, and Break into AI**

Instructor: [Andrew Ng](http://www.andrewng.org/)

 [Deep Learning Specialization on Coursera](https://www.coursera.org/specializations/deep-learning).

## Programming Assignments and Syllabus

### Course 1: Neural Networks and Deep Learning

  - [Week 2 - PA 1 - Logistic Regression with a Neural Network mindset](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb)
  : Build a logistic regression classifier to recognize cats from scratch (just one neuron flatten image and input to neuron ) 
  
  - [Week 3 - PA 2 - Planar data classification with one hidden layer](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb)
  : 2-class classification neural network with a single hidden layer (A hidden layer in an artificial neural network is a layer in between input layers and output layers). Try out diffrent hidden layer by changing number of neurons in layer 
  - [Week 4 - PA 3 - Building your Deep Neural Network: Step by StepÂ¶](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb)
  :To build your neural network, you will be implementing several "helper functions". These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network. (helper functions for 1.  Initialize the parameters 2. forward propagation 3. backward propagation 4.Compute the loss 5.update the parameters.)
  - [Week 4 - PA 4 - Deep Neural Network for Image Classification: Application](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Deep%20Neural%20Network%20-%20Application.ipynb)
  :will use use the functions implemented in the previous assignment to build a deep network, and apply it to cat vs non-cat classification. ( 2-layer neural network ,  L-layer deep neural network)
  

 - Syllabus
    -  Logistic Regression
    -  Computation graph
    -  Derivatives with a Computation Graph
    -  Vectorization
    -  Logistic Regression Gradient Descent
    -  Vectorizing Logistic Regression
    -  Random Initialization
    -  Activation functions
### Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

  - [Week 1 - PA 1 - Initialization](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Initialization.ipynb)
  - [Week 1 - PA 2 - Regularization](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Regularization.ipynb)
  - [Week 1 - PA 3 - Gradient Checking](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Gradient%20Checking.ipynb)
  - [Week 2 - PA 4 - Optimization Methods](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Optimization%20methods.ipynb)
  - [Week 3 - PA 5 - TensorFlow Tutorial](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Tensorflow%20Tutorial.ipynb)

### Course 3: Structuring Machine Learning Projects

  - There is no PA for this course. But this course comes with very interesting case study quizzes.
  
### Course 4: Convolutional Neural Networks

  - [Week 1 - PA 1 - Convolutional Model: step by step](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Step%20by%20Step%20-%20v1.ipynb)
  : implement convolutional (CONV) and pooling (POOL) layers in numpy, including both forward propagation and backward propagation
  - [Week 1 - PA 2 - Convolutional Model: application](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Application%20-%20v1.ipynb)
  : Implement a fully functioning ConvNet using TensorFlow on SIGNS dataset (SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5)
  - [Week 2 - PA 1 - Keras - Tutorial - Happy House](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Keras%20-%20Tutorial%20-%20Happy%20House%20v1.ipynb)
  :Learn to use Keras, a high-level neural networks API (programming framework), written in Python and capable of running on top of several lower-level frameworks including TensorFlow and CNTK . You have gathered pictures of your friends and yourself, taken by the front-door camera. The dataset is labbeled . classify the image as happy or not happy 
  - [Week 2 - PA 2 - Residual Networks](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Residual%20Networks%20-%20v1.ipynb)
  : Implement the basic building blocks of ResNets.
Put together these building blocks to implement and train a state-of-the-art neural network for image classification. (in keras )
  - [Week 3 - PA 1 - object detection ](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Autonomous_driving_application_Car_detection_v3a%20(1).ipynb)
  :Use object detection on a car detection dataset

  - [Week 4 - PA 1 - neural style transfer](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Art_Generation_with_Neural_Style_Transfer_v3a%20(1).ipynb)
  : Implement the neural style transfer algorithm
Generate novel artistic images using your algorithm
 
 
 
 - Syllabus
    -  Simple Convolutional Network Edge Detection Example
    -  Pooling Layers
    -  Strided Convolutions
    -  Padding
    -  ResNets Inception Network
    -  Object Detection Object Localization  Landmark Detection YOLO Algorithm
    -  face recognition Triplet Loss
    -  Siamese Network
    -  neural style transfer
  
  ### Resnet (Residual Network) and skip connection 
  The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the lower layers) to very complex features (at the deeper layers). However, using a deeper network doesn't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow. More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and "explode" to take very large values)
In ResNets, a "shortcut" or a "skip connection" allows the gradient to be directly backpropagated to earlier layers
  ### Neural style transfer
  it merges two images, namely: a "content" image (C) and a "style" image (S), to create a "generated" image (G) . Neural Style Transfer (NST) uses a previously trained convolutional network, and builds on top of that . These  models has already been trained on the very large ImageNet database, and thus has learned to recognize a variety of low level features (at the shallower layers) and high level features (at the deeper layers)
The shallower layers of a ConvNet tend to detect lower-level features such as edges and simple textures.
The deeper layers tend to detect higher-level features such as more complex textures as well as object classes


#### steps 
- Choose a "middle" activation layer
- Forward propagate image "C"
- Forward propagate image "G"
- Content Cost Function ![alt text](https://render.githubusercontent.com/render/math?math=J_%7Bcontent%7D%28C%2CG%29%20%3D%20%20%5Cfrac%7B1%7D%7B4%20%5Ctimes%20n_H%20%5Ctimes%20n_W%20%5Ctimes%20n_C%7D%5Csum%20_%7B%20%5Ctext%7Ball%20entries%7D%7D%20%28a%5E%7B%28C%29%7D%20-%20a%5E%7B%28G%29%7D%29%5E2%5Ctag%7B1%7D&mode=display)
- Gram matrix : In linear algebra, the Gram matrix G of a set of vectors $(v_{1},\dots ,v_{n})$ is the matrix of dot products, whose entries are ${\displaystyle G_{ij} = v_{i}^T v_{j} = np.dot(v_{i}, v_{j})  }$.
In other words, $G_{ij}$ compares how similar $v_i$ is to $v_j$: If they are highly similar, you would expect them to have a large dot product, and thus for $G_{ij}$ to be large
- Style cost : Your goal will be to minimize the distance between the Gram matrix of the "style" image S and the gram matrix of the "generated" image G


  ### Object Detection 
  "You Only Look Once" (YOLO) is a popular algorithm because it achieves high accuracy while also being able to run in real-time. This algorithm "only looks once" at the image in the sense that it requires only one forward propagation pass through the network to make predictions. After non-max suppression, it then outputs recognized objects together with the bounding boxes

#### summary of yolo 
- Input image (608, 608, 3)
- The input image goes through a CNN, resulting in a (19,19,5,85) dimensional output.
- After flattening the last two dimensions, the output is a volume of shape (19, 19, 425):
- Each cell in a 19x19 grid over the input image gives 425 numbers.
- 425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture.
- 85 = 5 + 80 where 5 is because $(p_c, b_x, b_y, b_h, b_w)$ has 5 numbers, and 80 is the number of classes we'd like to detect
- You then select only few boxes based on:
- Score-thresholding: throw away boxes that have detected a class with a score less than the threshold
- Non-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes
- This gives you YOLO's final output.
  
### Course 5: Sequence Models

  - [Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step](https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step%20-%20v2.ipynb)
  - [Week 1 - PA 2 - Character level language model - Dinosaurus land](https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Dinosaurus%20Island%20--%20Character%20level%20language%20model%20final%20-%20v3.ipynb)
