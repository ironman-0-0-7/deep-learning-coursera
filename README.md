# Deep Learning Specialization on Coursera

**Master Deep Learning, and Break into AI**

Instructor: [Andrew Ng](http://www.andrewng.org/)

 [Deep Learning Specialization on Coursera](https://www.coursera.org/specializations/deep-learning).

## Programming Assignments and Syllabus

- Course 1: Neural Networks and Deep Learning

  - [Week 2 - PA 1 - Logistic Regression with a Neural Network mindset](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb)
  : Build a logistic regression classifier to recognize cats from scratch (just one neuron flatten image and input to neuron ) 
  
  - [Week 3 - PA 2 - Planar data classification with one hidden layer](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb)
  : 2-class classification neural network with a single hidden layer (A hidden layer in an artificial neural network is a layer in between input layers and output layers). Try out diffrent hidden layer by changing number of neurons in layer 
  - [Week 4 - PA 3 - Building your Deep Neural Network: Step by StepÂ¶](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb)
  :To build your neural network, you will be implementing several "helper functions". These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network. (helper functions for 1.  Initialize the parameters 2. forward propagation 3. backward propagation 4.Compute the loss 5.update the parameters.)
  - [Week 4 - PA 4 - Deep Neural Network for Image Classification: Application](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Deep%20Neural%20Network%20-%20Application.ipynb)
  :will use use the functions implemented in the previous assignment to build a deep network, and apply it to cat vs non-cat classification. ( 2-layer neural network ,  L-layer deep neural network)
  

 - Syllabus
    -  Logistic Regression
    -  Computation graph
    -  Derivatives with a Computation Graph
    -  Vectorization
    -  Logistic Regression Gradient Descent
    -  Vectorizing Logistic Regression
    -  Random Initialization
    -  Activation functions
- Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

  - [Week 1 - PA 1 - Initialization](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Initialization.ipynb)
  - [Week 1 - PA 2 - Regularization](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Regularization.ipynb)
  - [Week 1 - PA 3 - Gradient Checking](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Gradient%20Checking.ipynb)
  - [Week 2 - PA 4 - Optimization Methods](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Optimization%20methods.ipynb)
  - [Week 3 - PA 5 - TensorFlow Tutorial](https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Tensorflow%20Tutorial.ipynb)

- Course 3: Structuring Machine Learning Projects

  - There is no PA for this course. But this course comes with very interesting case study quizzes.
  
- Course 4: Convolutional Neural Networks

  - [Week 1 - PA 1 - Convolutional Model: step by step](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Step%20by%20Step%20-%20v1.ipynb)
  : implement convolutional (CONV) and pooling (POOL) layers in numpy, including both forward propagation and backward propagation
  - [Week 1 - PA 2 - Convolutional Model: application](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Application%20-%20v1.ipynb)
  : Implement a fully functioning ConvNet using TensorFlow on SIGNS dataset (SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5)
  - [Week 2 - PA 1 - Keras - Tutorial - Happy House](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Keras%20-%20Tutorial%20-%20Happy%20House%20v1.ipynb)
  :Learn to use Keras, a high-level neural networks API (programming framework), written in Python and capable of running on top of several lower-level frameworks including TensorFlow and CNTK . You have gathered pictures of your friends and yourself, taken by the front-door camera. The dataset is labbeled . classify the image as happy or not happy 
  - [Week 2 - PA 2 - Residual Networks](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Residual%20Networks%20-%20v1.ipynb)
  : Implement the basic building blocks of ResNets.
Put together these building blocks to implement and train a state-of-the-art neural network for image classification. (in keras )
  - [Week 3 - PA 1 - object detection ](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Autonomous_driving_application_Car_detection_v3a%20(1).ipynb)
  - [Week 4 - PA 1 - neural style transfer](https://github.com/ironman-0-0-7/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Art_Generation_with_Neural_Style_Transfer_v3a%20(1).ipynb)
  : Implement the neural style transfer algorithm
Generate novel artistic images using your algorithm
 
 
 
 - Syllabus
    -  Simple Convolutional Network Edge Detection Example
    -  Pooling Layers
    -  Strided Convolutions
    -  Padding
    -  ResNets Inception Network
    -  Object Detection Object Localization  Landmark Detection YOLO Algorithm
    -  face recognition Triplet Loss
    -  Siamese Network
    -  neural style transfer
  
  ### Resnet (Residual Network) and skip connection 
  The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the lower layers) to very complex features (at the deeper layers). However, using a deeper network doesn't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow. More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and "explode" to take very large values)
In ResNets, a "shortcut" or a "skip connection" allows the gradient to be directly backpropagated to earlier layers
  
  
- Course 5: Sequence Models

  - [Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step](https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step%20-%20v2.ipynb)
  - [Week 1 - PA 2 - Character level language model - Dinosaurus land](https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Dinosaurus%20Island%20--%20Character%20level%20language%20model%20final%20-%20v3.ipynb)
